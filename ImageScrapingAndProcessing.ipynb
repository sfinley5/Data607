{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # send request to website\n",
    "from bs4 import BeautifulSoup as bs # convert the web content to bs object\n",
    "from bs4 import Comment # search if we are caught by Amazon as a robot\n",
    "from fake_useragent import UserAgent #create fake user agent from different browser\n",
    "import re # regular expression\n",
    "import pandas as pd # output dataframe\n",
    "import numpy as np # fast data manipulation\n",
    "import random # randomly use agent header for sending request\n",
    "import time #If access is denied, sleep 5s and then request again\n",
    "from collections import defaultdict #Used to declare a dictionary with emply \n",
    "print(requests.__version__)\n",
    "import os\n",
    "import csv\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a class to deal with web request and convert it to beautiful soup\n",
    "class get_soup:\n",
    "    header = None\n",
    "    #When the class is initiated, a list of user agent will be generated\n",
    "    '''\n",
    "    There is a pretty useful third-party package called fake-useragent \n",
    "    that provides a nice abstraction layer over user agents: https://pypi.org/project/fake-useragent/\n",
    "\n",
    "    If you don't want to use the local data, you can use the external data source to retrieve the user-agents. \n",
    "    #Set use_external_data to True:\n",
    "    '''\n",
    "    def __init__(self, total_user_agent = 1000):\n",
    "        ua = UserAgent(browsers=[\"chrome\", \"edge\", \"internet explorer\", \"firefox\", \"safari\", \"opera\"])\n",
    "        # I will generate a lsit of fake agent string with total number of total_user_agent\n",
    "        self.user_agent_set = set()\n",
    "        # Set a cap for user_agent_set to prevent endless loop\n",
    "        while(len(self.user_agent_set)<total_user_agent and len(self.user_agent_set) < 4500):\n",
    "            self.user_agent_set.add(ua.random)\n",
    "    '''\n",
    "    Define the function to get contents from each page. \n",
    "    Each header_attempts will use the same header until it is caught by the weg server.\n",
    "    In each header_attempts, we will try request_attempts times to request contents until we get the right contents\n",
    "    '''\n",
    "    def get_individual_soup(self, url, header_attempts = 10, request_attempts = 10):\n",
    "        self.soup = 'No Data Returned'\n",
    "        for _ in range(header_attempts):\n",
    "            request_count = 0\n",
    "            page = ''\n",
    "            notDenied = True\n",
    "            # We want to keep using the same header if that one particular header is working\n",
    "            # We change it unless it is recognized and banned by Web server\n",
    "            if get_soup.header is None:\n",
    "                user_agent = random.choice(list(self.user_agent_set))\n",
    "                get_soup.header = {'content-type': 'text/html;charset=UTF-8',\n",
    "                'Accept-Encoding': 'gzip, deflate, sdch',\n",
    "                'Accept-Language': 'en-US,en;q=0.8',\n",
    "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "                \"User-Agent\": user_agent}\n",
    "\n",
    "            while page == '' and request_count < request_attempts and notDenied:\n",
    "                try:\n",
    "                    request_count += 1\n",
    "                    page = requests.get(url, headers=get_soup.header, timeout=10)\n",
    "                    self.soup = bs(page.content, \"lxml\")\n",
    "                    '''If the page returns a message like To discuss automated access \n",
    "                        to Amazon data please contact api-services-support@amazon.com.\n",
    "                        We know we are denied access to the web page.\n",
    "                        Or,\n",
    "                        Amazon page blocks you by returning a login page\n",
    "                        In either case, lets try again using different header\n",
    "                    '''\n",
    "                    comments = self.soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "                    login_page = self.soup.find('a', id = 'createAccountSubmit', class_ = 'a-button-text')                    \n",
    "                    for comment in comments:\n",
    "                        if (\"api-services-support@amazon.com\" in comment) or login_page:\n",
    "                            notDenied = False\n",
    "                            get_soup.header = None\n",
    "                            self.soup = 'No Data Returned'\n",
    "                            break\n",
    "                            \n",
    "                    if (notDenied):\n",
    "                        return self.soup\n",
    "                    #We are caught by Web server as a bot, break this while and try a new header\n",
    "                    break\n",
    "                except:\n",
    "                    get_soup.header = None\n",
    "                    print(\"Connection refused by the server..\")\n",
    "                    print(\"Let me sleep for 5 seconds\")\n",
    "                    time.sleep(5)\n",
    "                    print(\"Now I will use a different header to request data...\")\n",
    "                    #The server does not respond to our request, break this while and try a new header\n",
    "                    break\n",
    "        return self.soup\n",
    "    '''\n",
    "    Customer Reviews, including Product Star Ratings, \n",
    "    help customers to learn more about the product and decide whether it is the right product for them.\n",
    "    To calculate the overall star rating and percentage breakdown by star, we don’t use a simple average. \n",
    "    Instead, our system considers things like how recent a review is and if the reviewer bought the item on Amazon. \n",
    "    It also analyses reviews to verify trustworthiness.\n",
    "    Learn more from\n",
    "    https://www.amazon.co.uk/gp/help/customer/display.html/ref=cm_cr_arp_d_omni_lm_btn?nodeId=G8UYX7LALQC8V9KA'''\n",
    "    #Define a function to get the review of a product on one page only\n",
    "    def get_page_reviews(self, ASIN, soup = None):\n",
    "        reviewlist = []\n",
    "        if soup is not None:\n",
    "            for item in soup.find_all('div', {'data-hook': 'review'}):\n",
    "                try:\n",
    "                    #This is domenstic review\n",
    "                    review = {\n",
    "                                'ASIN': ASIN,\n",
    "                                'product Name': soup.title.text.replace('Amazon.co.uk:Customer reviews:', '').strip(),\n",
    "                                'Review Title': item.find('a', {'data-hook': 'review-title'}).get_text().strip(),\n",
    "                                'Review Rating':  float(item.find('i', {'data-hook': 'review-star-rating'}).get_text().replace('out of 5 stars', '').strip()),\n",
    "                                'Review Body': item.find('span', {'data-hook': 'review-body'}).get_text().strip(),\n",
    "                                'Review Date': item.find('span', {'data-hook': 'review-date'}).get_text().strip(),\n",
    "                                }\n",
    "                except AttributeError:\n",
    "                    #This is international review\n",
    "                    try:\n",
    "                        review = {\n",
    "                                'ASIN': ASIN,\n",
    "                                'product Name': soup.title.text.replace('Amazon.co.uk:Customer reviews:', '').strip(),\n",
    "                                'Review Title': item.find('span', {'data-hook': 'review-title'}).get_text().strip(),\n",
    "                                'Review Rating':  float(item.find('i', {'data-hook': 'cmps-review-star-rating'}).get_text().replace('out of 5 stars', '').strip()),\n",
    "                                'Review Body': item.find('span', {'data-hook': 'review-body'}).get_text().strip(),\n",
    "                                'Review Date': item.find('span', {'data-hook': 'review-date'}).get_text().strip(),\n",
    "                                }\n",
    "                    except:\n",
    "                        #If there is still error, return None\n",
    "                        review = {\n",
    "                                'ASIN': None,\n",
    "                                'product Name': None,\n",
    "                                'Review Title': None,\n",
    "                                'Review Rating': None,\n",
    "                                'Review Body': None,\n",
    "                                'Review Date': None,\n",
    "                                }\n",
    "                reviewlist.append(review)\n",
    "        return reviewlist\n",
    "\n",
    "#Create a class to handle all the file I/O\n",
    "class Review_file_io:\n",
    "    '''\n",
    "    This method is to get the root link for each product\n",
    "    '''\n",
    "    @classmethod\n",
    "    def get_review_link(cls, file_loc):\n",
    "        #Get the review entrance link for all the product items\n",
    "        review_links = {}\n",
    "        with open (file_loc, mode = \"r\") as f:\n",
    "            for link in f:\n",
    "                entry_link = link.strip().split(\",\")[0]\n",
    "                if (not re.search(\"product-reviews/.*/ref\", entry_link)):\n",
    "                    continue\n",
    "                ASIN = re.search(\"product-reviews/.*/ref\", entry_link).group(0).split(\"/\")[1]\n",
    "                '''Need to think this again, this is mainly for empty page loc'''\n",
    "                if re.search(r'&pageNumber=\\d+$', entry_link):\n",
    "                    review_links[ASIN] = entry_link\n",
    "                else:\n",
    "                    review_links[ASIN] = entry_link + \"&pageNumber=\"\n",
    "        return review_links\n",
    "    '''\n",
    "    This method is to get all the reviews on every page of a product\n",
    "    '''\n",
    "    def get_product_reviews(self, file_loc, reviews_loc, empty_page_loc, total_page = 999, header_attempts = 3, request_attempts = 1):\n",
    "        review_links = Review_file_io.get_review_link(file_loc)\n",
    "        mySoup = get_soup()\n",
    "        empty_page = defaultdict(list)\n",
    "        reviews = []\n",
    "        #loop through each page and get reviews on each page\n",
    "        for ASIN, review_link in review_links.items():\n",
    "            for page_number in range(1,total_page):\n",
    "                print(f\"You are on product {ASIN} page {page_number}\")\n",
    "                page_url = f\"{review_link}{page_number}\"\n",
    "                page_soup = mySoup.get_individual_soup(page_url,header_attempts = header_attempts, request_attempts = request_attempts)\n",
    "                '''\n",
    "                There are 3 cases page_soup equals 'No Data Returned'.\n",
    "                1st is when you get caught by Amazon as a bot;\n",
    "                2nd is Amazon returns you a login page\n",
    "                3rd is when our scrapper has tried header_attempts*request_attempts times to reach the page,\n",
    "                    but still got nothing, either rejected or caught by the server;\n",
    "\n",
    "                There are case that you do get the page content from our web scrapper,\n",
    "                but there are no reviews on that page. For example, \n",
    "                1. You get the page, but the page \n",
    "                2. you hit the last review page;\n",
    "                3. the product item just does not have any reviews at all.\n",
    "                '''\n",
    "                if page_soup != 'No Data Returned':\n",
    "                    review = mySoup.get_page_reviews(ASIN, page_soup)\n",
    "                    #There are simply no reviews for this product item, there are 2 things can happen:\n",
    "                    #1st: the review page is just some random page returned by Amazon\n",
    "                    #2nd: the review page is a normal review page but \n",
    "                        #because the page number has gone out of bound, there is simply no review at all\n",
    "                    if not review:\n",
    "                        #this is is to check if the page is a normal review page but the page number is out of boundary\n",
    "                        #The first find is to check if the page still has product title\n",
    "                        #The second find is to check if there is no Previous Page or Next Page button, that means this is it, there is no more reviews to look, break it\n",
    "                        #what is inside this tag is: '←Previous pageNext page→'\n",
    "                        if page_soup.find(\"a\", attrs={\"data-hook\": \"product-link\"}) and not page_soup.find(\"ul\", {'class': 'a-pagination'}):\n",
    "                            break\n",
    "                        continue\n",
    "                        \n",
    "                    reviews.extend(review)\n",
    "\n",
    "                    #if not page_soup.find(\"ul\", {'class': 'a-pagination'}):\n",
    "                        #break\n",
    "                    #Last page is hit, we break the for loop\n",
    "                    if page_soup.find('li', {'class': 'a-disabled a-last'}):\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "                #When we failed to get the content for this page, record this page, and go to the next page\n",
    "                else:\n",
    "                    empty_page[ASIN].append(page_url)\n",
    "                    continue\n",
    "        #Save the reviews and empty page link\n",
    "        try:\n",
    "            with open (reviews_loc, mode = \"a\") as f:\n",
    "                csv_columns = ['ASIN', 'product Name', 'Review Title', 'Review Rating', 'Review Body', 'Review Date']\n",
    "                writer = csv.DictWriter(f, fieldnames=csv_columns)\n",
    "                writer.writeheader()\n",
    "                for prod_info in reviews:\n",
    "                    writer.writerow(prod_info)\n",
    "\n",
    "            with open (empty_page_loc, mode = \"a\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['URLs', 'ASIN'])\n",
    "                for key, page in empty_page.items():\n",
    "                    for link in page:\n",
    "                        writer.writerow([link, key])\n",
    "        except:\n",
    "            print(\"I/O error\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "mySoup = get_soup()\n",
    "\n",
    "def download(url, pathname):\n",
    "    \"\"\"\n",
    "    Downloads a file given an URL and puts it in the folder `pathname`\n",
    "    \"\"\"\n",
    "    # if path doesn't exist, make that path dir\n",
    "    if not os.path.isdir(pathname):\n",
    "        os.makedirs(pathname)\n",
    "    # download the body of response by chunk, not immediately\n",
    "    response = requests.get(url, stream=True)\n",
    "    # get the total file size\n",
    "    file_size = int(response.headers.get(\"Content-Length\", 0))\n",
    "    # get the file name\n",
    "    filename = os.path.join(pathname, url.split(\"/\")[-1])\n",
    "    # progress bar, changing the unit to bytes instead of iteration (default by tqdm)\n",
    "    progress = tqdm(response.iter_content(1024), f\"Downloading {filename}\", total=file_size, unit=\"B\", unit_scale=True, unit_divisor=1024)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        for data in progress.iterable:\n",
    "            # write data read to the file\n",
    "            f.write(data)\n",
    "            # update the progress bar manually\n",
    "            progress.update(len(data))\n",
    "\n",
    "def download_images(item, images):\n",
    "    for x in images:\n",
    "        download(x, f'/Users/stuartfinley/Downloads/{item}/')\n",
    "\n",
    "def download_pages(item, page):\n",
    "    soup = mySoup.get_individual_soup(f'https://www.amazon.fr/s?k={item}&page={page}',\n",
    "                                      header_attempts=2, request_attempts=1)\n",
    "    imagesList = []\n",
    "    #If there is nothing return from the website, return an empty list\n",
    "    if soup != 'No Data Returned':\n",
    "        if (mySoup.header is not None):\n",
    "            images = soup.find_all('img', attrs={'class': 's-image'})\n",
    "            for x in images:\n",
    "                imagesList.append(x['src'])\n",
    "    else:\n",
    "        print(f\"No data returned. You are using `{mySoup.header}` to retrieve data\")\n",
    "    return imagesList\n",
    "\n",
    "def main():\n",
    "    items = [\"shoes\",\n",
    "            \"sunglasses\",\n",
    "            \"phone\",\n",
    "            \"pens\",\n",
    "            \"tshirt\",\n",
    "            \"shorts\",\n",
    "            \"watches\",\n",
    "            \"chair\",\n",
    "            \"hat\",\n",
    "            \"bottle\",\n",
    "            \"speaker\",\n",
    "            \"earbuds\",\n",
    "            \"address_data\"]\n",
    "    \n",
    "    for item in items:\n",
    "        imagesList = []\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            for page in range(1, 20):\n",
    "                imagesList += executor.submit(download_pages, item, page).result()\n",
    "        \n",
    "        # Remove duplicates\n",
    "        final = list(set(imagesList))\n",
    "\n",
    "        # Create threads for downloading images\n",
    "        threads = []\n",
    "        for i in final:\n",
    "            t = threading.Thread(target=download, args=(i, f'/Users/stuartfinley/Downloads/untitled folder 2/{item}/'))\n",
    "            threads.append(t)\n",
    "\n",
    "        # Start all threads\n",
    "        for t in threads:\n",
    "            t.start()\n",
    "\n",
    "        # Wait for all threads to finish\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "print(os.path.exists(\"/Users/stuartfinley/Downloads/ImageFolder2/\"))\n",
    "\n",
    "def rename_files(path):\n",
    "    for root, dirs, files in os.walk(path, topdown=False):\n",
    "        for name in files:\n",
    "            if name.endswith(\".jpg\"):\n",
    "                old_path = os.path.join(root, name)\n",
    "                new_name = name.split(\".\")[0] + \".jpg\"\n",
    "                print(new_name)\n",
    "                new_path = os.path.join(root, new_name)\n",
    "                os.rename(old_path, new_path)\n",
    "\n",
    "rename_files(\"/Users/stuartfinley/Downloads/ImageFolder2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import concurrent.futures\n",
    "from numpy import expand_dims\n",
    "from tensorflow.keras.utils import load_img\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Define the folder path where the images are stored\n",
    "folder_path = \"/Users/stuartfinley/Downloads/ImageFolder2/\"\n",
    "\n",
    "def process_file(file_path):\n",
    "    # load the image\n",
    "    img = load_img(file_path)\n",
    "\n",
    "    # convert to numpy array\n",
    "    data = img_to_array(img)\n",
    "\n",
    "    # expand dimension to one sample\n",
    "    samples = expand_dims(data, 0)\n",
    "\n",
    "    # create image data augmentation generator\n",
    "    datagen = ImageDataGenerator(height_shift_range=0.35)\n",
    "\n",
    "    # prepare iterator\n",
    "    it = datagen.flow(samples, batch_size=1)\n",
    "\n",
    "    # create the augmented subfolder if it does not exist\n",
    "    if not os.path.exists(os.path.join(os.path.dirname(file_path), \"augmented\")):\n",
    "        os.mkdir(os.path.join(os.path.dirname(file_path), \"augmented\"))\n",
    "\n",
    "    # generate samples and save\n",
    "    for i in range(9):\n",
    "        # generate batch of images\n",
    "        batch = it.next()\n",
    "\n",
    "        # convert to unsigned integers for viewing\n",
    "        image = batch[0].astype('uint8')\n",
    "\n",
    "        # generate a timestamp\n",
    "        timestamp = int(time.time() * 1000)\n",
    "\n",
    "        # save the image to the augmented subfolder with a timestamp in the filename\n",
    "        save_path = os.path.join(os.path.dirname(file_path), \"augmented\", f\"augmented_image_{i}_{timestamp}.jpg\")\n",
    "        pyplot.imsave(save_path, image)\n",
    "\n",
    "# Loop through all the files in the main images folder and each of the sub folders\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            # Submit a new task to the executor for each file found\n",
    "            executor.submit(process_file, os.path.join(root, file))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
